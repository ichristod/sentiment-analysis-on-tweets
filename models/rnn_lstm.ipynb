{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64a5327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh really don't wanna be awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491755</th>\n",
       "      <td>0</td>\n",
       "      <td>Trying to amuse my cousin. It's not working! a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470924</th>\n",
       "      <td>0</td>\n",
       "      <td>@JonasAustralia  i wanted to win! congrats to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491263</th>\n",
       "      <td>0</td>\n",
       "      <td>That's it!! I can't take it no more!! After su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836489</th>\n",
       "      <td>4</td>\n",
       "      <td>@beckybootsx i hope your not drinking alcohol!...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                    original_tweets\n",
       "128037          0                    Oh really don't wanna be awake \n",
       "491755          0  Trying to amuse my cousin. It's not working! a...\n",
       "470924          0  @JonasAustralia  i wanted to win! congrats to ...\n",
       "491263          0  That's it!! I can't take it no more!! After su...\n",
       "836489          4  @beckybootsx i hope your not drinking alcohol!..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "#jupyter path\n",
    "cols = ['sentiment','id','date','query_string','user','original_tweets']\n",
    "df_encoding = \"ISO-8859-1\"\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding =df_encoding, header=None, names=cols)\n",
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "df = df.sample(frac=0.02, replace=True, random_state=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c9332",
   "metadata": {},
   "source": [
    "## Define usefull functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8caf60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ichristod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ichristod/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# set stop words for english language\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punctuations(text):\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    list_of_words = [word for word in text.split(' ') if word not in stop_words]\n",
    "    words_to_text = \" \".join(list_of_words)\n",
    "    return words_to_text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    clear_text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return clear_text\n",
    "\n",
    "def do_lem(text):\n",
    "    text = WordNetLemmatizer().lemmatize(text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):    \n",
    "    # regex dictionary\n",
    "    regex = {\n",
    "        \"urls\": r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",\n",
    "        \"mentions\": r\"@[A-Za-z0-9]+\",\n",
    "        \"hashtags\": r\"#[A-Za-z0-9]+\",\n",
    "        \"whitespaces\": \"\\s+\"\n",
    "    }\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(regex['urls'], '', text)\n",
    "    text = re.sub(regex['mentions'], '', text)\n",
    "    text = re.sub(regex['hashtags'], '', text)\n",
    "    text = do_lem(text)\n",
    "    text = remove_stopwords(text) \n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub(regex['whitespaces'], ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_subsets(dataset, features, labels, num_classes, train_size=0, valid_size=0, test_size=0):\n",
    "    subsets = {}\n",
    "    \n",
    "    # Define a size for your train set \n",
    "    train_n = int(train_size * len(dataset))\n",
    "    valid_n = int(valid_size * len(dataset))\n",
    "    test_n = int(test_size * len(dataset))\n",
    "    \n",
    "    #train_test split\n",
    "    subsets['X_train'] = dataset[features][:train_n]\n",
    "    subsets['X_valid'] = dataset[features][train_n:train_n+valid_n]\n",
    "    subsets['X_test'] = dataset[features][train_n+valid_n:train_n+valid_n+test_n]\n",
    "\n",
    "    # Categorically encode labels\n",
    "    subsets['Y_train'] = to_categorical(dataset[labels][:train_n].values, num_classes)\n",
    "    subsets['Y_valid'] = to_categorical(dataset[labels][train_n:train_n+valid_n].values, num_classes)\n",
    "    subsets['Y_test'] = to_categorical(dataset[labels][train_n+valid_n:train_n+valid_n+test_n].values, num_classes)\n",
    "    return subsets\n",
    "\n",
    "def words_to_sequences(max_sentence_length, subsets):\n",
    "    seq_subsets = {}\n",
    "    vocab_size = 0\n",
    "    tokenizer = Tokenizer()\n",
    "    for key, value in subsets.items():\n",
    "        if key.startswith('X'):\n",
    "            # create vocabulary based on word frequency\n",
    "            #   -word_counts: Dictionary of words and their corresponding counts.\n",
    "            #   -word_docs: Dictionary of words and their corresponding documents appeared in.\n",
    "            #   -word_index: Dictionary of words and their uniquely assigned integers.\n",
    "            #   -document_count: Count of the total number of documents that were used to fit the Tokenizer.\n",
    "            if key.startswith('X_train'):\n",
    "                tokenizer.fit_on_texts(list(value))\n",
    "            \n",
    "            # texts_to_sequences assigns integers to words for each document\n",
    "            sequence = tokenizer.texts_to_sequences(value)\n",
    "            # padding to prepare sequences of same length\n",
    "            sequence = pad_sequences(sequence, maxlen = max_sentence_length)\n",
    "            seq_subsets[key] = sequence\n",
    "            \n",
    "            if len(tokenizer.word_index) > vocab_size:\n",
    "                vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "    return seq_subsets, vocab_size+1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533a4ba",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f19d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max chars in a tweet: 317\n",
      "max num of words in a tweet: 29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original_tweets</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh really don't wanna be awake</td>\n",
       "      <td>oh really wanna awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491755</th>\n",
       "      <td>0</td>\n",
       "      <td>Trying to amuse my cousin. It's not working! a...</td>\n",
       "      <td>trying amuse cousin working hes playing halo wo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470924</th>\n",
       "      <td>0</td>\n",
       "      <td>@JonasAustralia  i wanted to win! congrats to ...</td>\n",
       "      <td>wanted win congrats anyways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491263</th>\n",
       "      <td>0</td>\n",
       "      <td>That's it!! I can't take it no more!! After su...</td>\n",
       "      <td>thats it cant take more summer school im talki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836489</th>\n",
       "      <td>1</td>\n",
       "      <td>@beckybootsx i hope your not drinking alcohol!...</td>\n",
       "      <td>hope drinking alcohol lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053847</th>\n",
       "      <td>1</td>\n",
       "      <td>Breakfast with my mommy</td>\n",
       "      <td>breakfast mommy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992577</th>\n",
       "      <td>1</td>\n",
       "      <td>1 tut down, 123981 projects to go!!</td>\n",
       "      <td>tut down projects go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275825</th>\n",
       "      <td>0</td>\n",
       "      <td>@melody1976 I'm jealous!!  I have 4 weeks to w...</td>\n",
       "      <td>im jealous weeks wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501507</th>\n",
       "      <td>0</td>\n",
       "      <td>'s heart is aching</td>\n",
       "      <td>s heart aching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314240</th>\n",
       "      <td>0</td>\n",
       "      <td>just emptied out the bird box, 6 dead baby blu...</td>\n",
       "      <td>emptied bird box dead baby blue tits sad handf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                    original_tweets  \\\n",
       "128037           0                    Oh really don't wanna be awake    \n",
       "491755           0  Trying to amuse my cousin. It's not working! a...   \n",
       "470924           0  @JonasAustralia  i wanted to win! congrats to ...   \n",
       "491263           0  That's it!! I can't take it no more!! After su...   \n",
       "836489           1  @beckybootsx i hope your not drinking alcohol!...   \n",
       "...            ...                                                ...   \n",
       "1053847          1                           Breakfast with my mommy    \n",
       "992577           1               1 tut down, 123981 projects to go!!    \n",
       "275825           0  @melody1976 I'm jealous!!  I have 4 weeks to w...   \n",
       "501507           0                                's heart is aching    \n",
       "314240           0  just emptied out the bird box, 6 dead baby blu...   \n",
       "\n",
       "                                                    tweets  \n",
       "128037                               oh really wanna awake  \n",
       "491755     trying amuse cousin working hes playing halo wo  \n",
       "470924                         wanted win congrats anyways  \n",
       "491263   thats it cant take more summer school im talki...  \n",
       "836489                           hope drinking alcohol lol  \n",
       "...                                                    ...  \n",
       "1053847                                    breakfast mommy  \n",
       "992577                                tut down projects go  \n",
       "275825                               im jealous weeks wait  \n",
       "501507                                      s heart aching  \n",
       "314240   emptied bird box dead baby blue tits sad handf...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy dataframe\n",
    "df_clean = df.copy(deep=True)\n",
    "\n",
    "# clean text\n",
    "df_clean['tweets'] = df_clean['original_tweets'].apply(clean_text)\n",
    "\n",
    "# transform labels\n",
    "df_clean['sentiment'] = df_clean['sentiment'].apply(lambda x: x if x<4 else 1)\n",
    "\n",
    "# keep max length of words and sentences\n",
    "words_length = max(len(w) for w in df_clean['tweets'])\n",
    "sentence_length = max(len(w.split(' ')) for w in df_clean['tweets'])\n",
    "\n",
    "print(\"max chars in a tweet:\", words_length)\n",
    "print(\"max num of words in a tweet:\", sentence_length)\n",
    "\n",
    "df_clean.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43f00a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh really wanna awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>trying amuse cousin working hes playing halo wo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>wanted win congrats anyways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>thats it cant take more summer school im talki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hope drinking alcohol lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>0</td>\n",
       "      <td>neither knees give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>1</td>\n",
       "      <td>perfect wedding perfect couple perfect shoot l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>0</td>\n",
       "      <td>piggie lucky find hugs oxox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>1</td>\n",
       "      <td>ooh sorry last reply meant update song about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>0</td>\n",
       "      <td>lost please help find good home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                             tweets\n",
       "0              0                              oh really wanna awake\n",
       "1              0    trying amuse cousin working hes playing halo wo\n",
       "2              0                        wanted win congrats anyways\n",
       "3              0  thats it cant take more summer school im talki...\n",
       "4              1                          hope drinking alcohol lol\n",
       "...          ...                                                ...\n",
       "31995          0                                 neither knees give\n",
       "31996          1  perfect wedding perfect couple perfect shoot l...\n",
       "31997          0                        piggie lucky find hugs oxox\n",
       "31998          1       ooh sorry last reply meant update song about\n",
       "31999          0                    lost please help find good home\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove necessary columns & reset indexes\n",
    "df_clean.drop(['original_tweets'],axis=1,inplace=True)\n",
    "df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1e56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import defaultdict\n",
    "\n",
    "features = 'tweets'\n",
    "labels = 'sentiment'\n",
    "\n",
    "#create appropriate subsets\n",
    "initial_subsets = create_subsets(dataset=df_clean, features=features, labels=labels, \n",
    "                   train_size=0.6, valid_size=0.2, test_size=0.2, num_classes=2)\n",
    "\n",
    "# convert features (text) to sequences\n",
    "seq_subsets, vocab_size = words_to_sequences(sentence_length, subsets=initial_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a10be0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 29, 48)            1073424   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 120)               81120     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 242       \n",
      "=================================================================\n",
      "Total params: 1,154,786\n",
      "Trainable params: 1,154,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "192/192 [==============================] - 25s 37ms/step - loss: 0.6875 - accuracy: 0.5447 - val_loss: 0.6248 - val_accuracy: 0.6709\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.5999 - accuracy: 0.6851 - val_loss: 0.5738 - val_accuracy: 0.7009\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.5426 - accuracy: 0.7277 - val_loss: 0.5602 - val_accuracy: 0.7088\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 8s 42ms/step - loss: 0.5078 - accuracy: 0.7533 - val_loss: 0.5466 - val_accuracy: 0.7209\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 7s 37ms/step - loss: 0.4881 - accuracy: 0.7725 - val_loss: 0.5436 - val_accuracy: 0.7233\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 8s 41ms/step - loss: 0.4692 - accuracy: 0.7825 - val_loss: 0.5538 - val_accuracy: 0.7250\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.4455 - accuracy: 0.7965 - val_loss: 0.5425 - val_accuracy: 0.7330\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 0.4235 - accuracy: 0.8089 - val_loss: 0.5483 - val_accuracy: 0.7306\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 8s 43ms/step - loss: 0.4146 - accuracy: 0.8183 - val_loss: 0.5519 - val_accuracy: 0.7309\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.3974 - accuracy: 0.8240 - val_loss: 0.5619 - val_accuracy: 0.7280\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 8s 41ms/step - loss: 0.3841 - accuracy: 0.8289 - val_loss: 0.5725 - val_accuracy: 0.7328\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.3675 - accuracy: 0.8430 - val_loss: 0.5696 - val_accuracy: 0.7295\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.3553 - accuracy: 0.8456 - val_loss: 0.5995 - val_accuracy: 0.7295\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.3492 - accuracy: 0.8510 - val_loss: 0.6058 - val_accuracy: 0.7253\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 8s 39ms/step - loss: 0.3270 - accuracy: 0.8611 - val_loss: 0.6452 - val_accuracy: 0.7256\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 0.3203 - accuracy: 0.8631 - val_loss: 0.6314 - val_accuracy: 0.7234\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.3078 - accuracy: 0.8749 - val_loss: 0.6349 - val_accuracy: 0.7202\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 7s 38ms/step - loss: 0.2912 - accuracy: 0.8815 - val_loss: 0.6434 - val_accuracy: 0.7208\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 8s 42ms/step - loss: 0.2780 - accuracy: 0.8854 - val_loss: 0.6465 - val_accuracy: 0.7178\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 8s 40ms/step - loss: 0.2807 - accuracy: 0.8844 - val_loss: 0.7262 - val_accuracy: 0.7177\n",
      "200/200 [==============================] - 2s 6ms/step - loss: 0.7120 - accuracy: 0.7250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7119863033294678, 0.7250000238418579]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import GRU\n",
    "\n",
    "\n",
    "embed_dim = 48\n",
    "lstm_out = 120\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=sentence_length,trainable=True)) \n",
    "model.add(LSTM(lstm_out, dropout=0.2,recurrent_dropout=0.2))\n",
    "#model.add(GRU(100))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='Adamax',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(seq_subsets['X_train'], initial_subsets['Y_train'],validation_data = (seq_subsets['X_valid'],initial_subsets['Y_valid']),epochs = 20, batch_size=100)\n",
    "model.evaluate(seq_subsets['X_test'],initial_subsets['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bd1ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(model.layers[0].get_weights()[0][1,2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b02439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
